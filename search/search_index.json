{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : Rahma Nur Layla Sari NIM : 160411100104 Mata Kuliah : Penambangan dan Pencarian Web Crawling Data pada WEB Target Website : https://www.visitkorea.or.id/category/informasi-umum/ Yang dibutuhkan utnuk menjalankan program adalah Python 3.6 dengan Library yang di install pada cmd : \u00a7 Request \u00a7 BeautifulSoup4 \u00a7 SQLite 3 (library yang tersedia pada python) \u00a7 Csv (library yang tersedia pada python) \u00a7 Numpy \u00a7 Scipy \u00a7 Scikit-learn \u00a7 Scikit Fuzzy Crawling Data pada WEB : Mengambil data dari sebuah halaman web dan mengelompokkan data yang sudah di crawling menggunakan metode clustering.Crawling digunakan untuk mengambil data yang berupa text, citra, audio, video, dll. Code Program sebagai berikut : a. Membuat database dengan SQLite3 dan dikoneksikan pada sqlite database yang bersama DB_korea koneksi = sqlite3.connect('DB_korea.db') koneksi.execute(''' CREATE TABLE if not exists VisitKorea (judul TEXT NOT NULL, isi TEXT NOT NULL);''') b. Menentukan link website yang akan di crawl dan disimpan di variable src, kemudian crawling next page apa saja yang akan diambil. Pada web yang akan diambil kali ini, kita mengambil judul, paragraf, dan isi yang ada pada web tersebut. link = artikel[i].find('a')['href'] \u200b page = requests.get(link) \u200b soup = BeautifulSoup(page.content, 'html.parser') \u200b judul = soup.find(class_='title mb-3 text-center').getText() \u200b isi = soup.find(class_='entry-page') \u200b paragraf = isi.findAll('p') \u200b p = ' ' c. kemudian setiap isi, diambil judul dan isi paragraf lantas dicari tag p cek = koneksi.execute(\"SELECT * FROM VisitKorea WHERE judul=?\", (judul,)) cek = cek.fetchall() if (len(cek) == 0): \u200b koneksi.execute('INSERT INTO VisitKorea values (?,?)', (judul, p)); CODE PROGRAM ** artikel = soup.find(class_='row tag-page') artikel = artikel.findAll(class_='col-md-6 col-xl-4') koneksi = sqlite3.connect('DB_korea.db') koneksi.execute(''' CREATE TABLE if not exists VisitKorea (judul TEXT NOT NULL, isi TEXT NOT NULL);''') for i in range(len(artikel)): link = artikel[i].find('a')['href'] page = requests.get(link) soup = BeautifulSoup(page.content, 'html.parser') judul = soup.find(class_='title mb-3 text-center').getText() isi = soup.find(class_='entry-page') paragraf = isi.findAll('p') p = '' for s in paragraf: p+=s.getText() +' ' cek = koneksi.execute(\"SELECT * FROM VisitKorea WHERE judul=?\", (judul,)) cek = cek.fetchall() if (len(cek) == 0): koneksi.execute('INSERT INTO VisitKorea values (?,?)', (judul, p)); Pre-Processing pada tahap pre-processing dilakukan 3 tahapan, yaitu : tahap Stopword Removal (Menghilangkan kata dan tanda baca yang terlalu sering muncul) Stemming (Menggunakan suatu kata menjadi kata dasar) Tokenisasi n-gram (memecah kalimat per kata) Matrik VSM Merupak proses perhitungan kemunculan semua kata yang terdapat pada suata kata Code conn = sqlite3.connect('DB_korea.db') matrix2=[] cursor = conn.execute(\"SELECT* from VisitKOrea\") for row in cursor: \u200b tampung = [] \u200b for i in berhasil: \u200b tampung.append(row[2].lower().count(i)) \u200b #print(tampung) \u200b #print(row[2]) \u200b matrix2.append(tampung) print(matrix2) with open ('data_matrix.csv', newline='', mode='w')as employee_file : \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in matrix : \u200b employee_writer.writerow(i) TF-IDF Digunakan untuk mencari banyak kata yang muncul pada satu dokumen maka IDF digunakan untuk mengetahui mengetahui dokumen yang mana saja yang memiliki kata kata yang sama CODE tf-idf df = list() for d in range (len(matrix[0])): \u200b total = 0 \u200b for i in range(len(matrix)): \u200b if matrix[i][d] !=0: \u200b total += 1 \u200b df.append(total) idf = list() for i in df: \u200b tmp = 1 + log10(len(matrix)/(1+i)) \u200b idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): \u200b tampungBaris = [] \u200b for kolom in range(len(matrix[0])): \u200b tmp = tf[baris][kolom] * idf[kolom] \u200b tampungBaris.append(tmp) \u200b tfidf.append(tampungBaris) with open('tf-idf.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in tfidf: \u200b employee_writer.writerow(i) Code untuk ketiga tahap Stopword Removal (Menghilangkan kata dan tanda baca yang terlalu sering muncul)Stemming (Menggunakan suatu kata menjadi kata dasar) Tokenisasi n-gram (memecah kalimat per kata) Code Program def pearsonCalculate(data, u,v): \u200b \"i, j is an index\" \u200b atas=0; bawah_kiri=0; bawah_kanan = 0 \u200b for k in range(len(data)): \u200b atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) \u200b bawah_kiri += (data[k,u] - meanFitur[u]) 2 \u200b bawah_kanan += (data[k,v] - meanFitur[v]) 2 \u200b bawah_kiri = bawah_kiri 0.5 \u200b bawah_kanan = bawah_kanan 0.5 \u200b return atas/(bawah_kiri * bawah_kanan) def meanF(data): \u200b meanFitur=[] \u200b for i in range(len(data[0])): \u200b meanFitur.append(sum(data[:,i])/len(data)) \u200b return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): \u200b global meanFitur \u200b data = np.array(data) \u200b meanFitur = meanF(data) \u200b u=0 \u200b while u < len(data[0]): \u200b dataBaru=data[:, :u+1] \u200b meanBaru=meanFitur[:u+1] \u200b katadasarBaru=katadasar[:u+1] \u200b v = u \u200b while v < len(data[0]): \u200b if u != v: \u200b value = pearsonCalculate(data, u,v) \u200b if value < threshold: \u200b dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) \u200b meanBaru = np.hstack((meanBaru, meanFitur[v])) \u200b katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) \u200b v+=1 \u200b data = dataBaru \u200b meanFitur=meanBaru \u200b katadasar = katadasarBaru \u200b if u%50 == 0 : print(\"proses : \", u, data.shape) \u200b u+=1 \u200b return katadasar, data Clustering Pengelompokkan data menjadi k-kelompok. K-Means merupakan salah satu metode pengelompokkan data yang berusaha mempartisi data yang ada ke dalam bentuk dua atau lebih. CODE PROGRAM katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf,0.783); kmeans = KMeans(n_clusters=3, random_state=0).fit(tfidf); print(kmeans.labels_) classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print (s_avg) KESIMPULAN Menggunakan seleksi fitur dan Clustering dengan Fuzzy K-Mean merupakan metode terbaik dan akurat","title":"Home"},{"location":"#crawling-data-pada-web","text":"Target Website : https://www.visitkorea.or.id/category/informasi-umum/ Yang dibutuhkan utnuk menjalankan program adalah Python 3.6 dengan Library yang di install pada cmd : \u00a7 Request \u00a7 BeautifulSoup4 \u00a7 SQLite 3 (library yang tersedia pada python) \u00a7 Csv (library yang tersedia pada python) \u00a7 Numpy \u00a7 Scipy \u00a7 Scikit-learn \u00a7 Scikit Fuzzy","title":"Crawling Data pada WEB"},{"location":"#crawling-data-pada-web_1","text":"Mengambil data dari sebuah halaman web dan mengelompokkan data yang sudah di crawling menggunakan metode clustering.Crawling digunakan untuk mengambil data yang berupa text, citra, audio, video, dll. Code Program sebagai berikut : a. Membuat database dengan SQLite3 dan dikoneksikan pada sqlite database yang bersama DB_korea koneksi = sqlite3.connect('DB_korea.db') koneksi.execute(''' CREATE TABLE if not exists VisitKorea (judul TEXT NOT NULL, isi TEXT NOT NULL);''') b. Menentukan link website yang akan di crawl dan disimpan di variable src, kemudian crawling next page apa saja yang akan diambil. Pada web yang akan diambil kali ini, kita mengambil judul, paragraf, dan isi yang ada pada web tersebut. link = artikel[i].find('a')['href'] \u200b page = requests.get(link) \u200b soup = BeautifulSoup(page.content, 'html.parser') \u200b judul = soup.find(class_='title mb-3 text-center').getText() \u200b isi = soup.find(class_='entry-page') \u200b paragraf = isi.findAll('p') \u200b p = ' ' c. kemudian setiap isi, diambil judul dan isi paragraf lantas dicari tag p cek = koneksi.execute(\"SELECT * FROM VisitKorea WHERE judul=?\", (judul,)) cek = cek.fetchall() if (len(cek) == 0): \u200b koneksi.execute('INSERT INTO VisitKorea values (?,?)', (judul, p)); CODE PROGRAM ** artikel = soup.find(class_='row tag-page') artikel = artikel.findAll(class_='col-md-6 col-xl-4') koneksi = sqlite3.connect('DB_korea.db') koneksi.execute(''' CREATE TABLE if not exists VisitKorea (judul TEXT NOT NULL, isi TEXT NOT NULL);''') for i in range(len(artikel)): link = artikel[i].find('a')['href'] page = requests.get(link) soup = BeautifulSoup(page.content, 'html.parser') judul = soup.find(class_='title mb-3 text-center').getText() isi = soup.find(class_='entry-page') paragraf = isi.findAll('p') p = '' for s in paragraf: p+=s.getText() +' ' cek = koneksi.execute(\"SELECT * FROM VisitKorea WHERE judul=?\", (judul,)) cek = cek.fetchall() if (len(cek) == 0): koneksi.execute('INSERT INTO VisitKorea values (?,?)', (judul, p));","title":"Crawling Data pada WEB :"},{"location":"#pre-processing","text":"pada tahap pre-processing dilakukan 3 tahapan, yaitu : tahap Stopword Removal (Menghilangkan kata dan tanda baca yang terlalu sering muncul) Stemming (Menggunakan suatu kata menjadi kata dasar) Tokenisasi n-gram (memecah kalimat per kata)","title":"Pre-Processing"},{"location":"#matrik-vsm","text":"Merupak proses perhitungan kemunculan semua kata yang terdapat pada suata kata Code conn = sqlite3.connect('DB_korea.db') matrix2=[] cursor = conn.execute(\"SELECT* from VisitKOrea\") for row in cursor: \u200b tampung = [] \u200b for i in berhasil: \u200b tampung.append(row[2].lower().count(i)) \u200b #print(tampung) \u200b #print(row[2]) \u200b matrix2.append(tampung) print(matrix2) with open ('data_matrix.csv', newline='', mode='w')as employee_file : \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in matrix : \u200b employee_writer.writerow(i)","title":"Matrik VSM"},{"location":"#tf-idf","text":"Digunakan untuk mencari banyak kata yang muncul pada satu dokumen maka IDF digunakan untuk mengetahui mengetahui dokumen yang mana saja yang memiliki kata kata yang sama CODE","title":"TF-IDF"},{"location":"#tf-idf_1","text":"df = list() for d in range (len(matrix[0])): \u200b total = 0 \u200b for i in range(len(matrix)): \u200b if matrix[i][d] !=0: \u200b total += 1 \u200b df.append(total) idf = list() for i in df: \u200b tmp = 1 + log10(len(matrix)/(1+i)) \u200b idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): \u200b tampungBaris = [] \u200b for kolom in range(len(matrix[0])): \u200b tmp = tf[baris][kolom] * idf[kolom] \u200b tampungBaris.append(tmp) \u200b tfidf.append(tampungBaris) with open('tf-idf.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in tfidf: \u200b employee_writer.writerow(i) Code untuk ketiga tahap Stopword Removal (Menghilangkan kata dan tanda baca yang terlalu sering muncul)Stemming (Menggunakan suatu kata menjadi kata dasar) Tokenisasi n-gram (memecah kalimat per kata) Code Program def pearsonCalculate(data, u,v): \u200b \"i, j is an index\" \u200b atas=0; bawah_kiri=0; bawah_kanan = 0 \u200b for k in range(len(data)): \u200b atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) \u200b bawah_kiri += (data[k,u] - meanFitur[u]) 2 \u200b bawah_kanan += (data[k,v] - meanFitur[v]) 2 \u200b bawah_kiri = bawah_kiri 0.5 \u200b bawah_kanan = bawah_kanan 0.5 \u200b return atas/(bawah_kiri * bawah_kanan) def meanF(data): \u200b meanFitur=[] \u200b for i in range(len(data[0])): \u200b meanFitur.append(sum(data[:,i])/len(data)) \u200b return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): \u200b global meanFitur \u200b data = np.array(data) \u200b meanFitur = meanF(data) \u200b u=0 \u200b while u < len(data[0]): \u200b dataBaru=data[:, :u+1] \u200b meanBaru=meanFitur[:u+1] \u200b katadasarBaru=katadasar[:u+1] \u200b v = u \u200b while v < len(data[0]): \u200b if u != v: \u200b value = pearsonCalculate(data, u,v) \u200b if value < threshold: \u200b dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) \u200b meanBaru = np.hstack((meanBaru, meanFitur[v])) \u200b katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) \u200b v+=1 \u200b data = dataBaru \u200b meanFitur=meanBaru \u200b katadasar = katadasarBaru \u200b if u%50 == 0 : print(\"proses : \", u, data.shape) \u200b u+=1 \u200b return katadasar, data","title":"tf-idf"},{"location":"#clustering","text":"Pengelompokkan data menjadi k-kelompok. K-Means merupakan salah satu metode pengelompokkan data yang berusaha mempartisi data yang ada ke dalam bentuk dua atau lebih. CODE PROGRAM katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf,0.783); kmeans = KMeans(n_clusters=3, random_state=0).fit(tfidf); print(kmeans.labels_) classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print (s_avg)","title":"Clustering"},{"location":"#kesimpulan","text":"Menggunakan seleksi fitur dan Clustering dengan Fuzzy K-Mean merupakan metode terbaik dan akurat","title":"KESIMPULAN"}]}