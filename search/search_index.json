{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Nama : Rahma Nur Layla Sari NIM : 160411100104 Mata Kuliah : Penambangan dan Pencarian Web","title":"Home"},{"location":"web_content/","text":"Nama : Rahma Nur Layla Sari NIM : 160411100104 Mata Kuliah : Penambangan dan Pencarian Web Crawling Data pada WEB Target Website : https://www.visitkorea.or.id/category/informasi-umum/ Yang dibutuhkan utnuk menjalankan program adalah Python 3.6 dengan Library yang di install pada cmd : \u00a7 Request \u00a7 BeautifulSoup4 \u00a7 SQLite 3 (library yang tersedia pada python) \u00a7 Csv (library yang tersedia pada python) \u00a7 Numpy \u00a7 Scipy \u00a7 Scikit-learn \u00a7 Scikit Fuzzy Crawling Data pada WEB : Mengambil data dari sebuah halaman web dan mengelompokkan data yang sudah di crawling menggunakan metode clustering.Crawling digunakan untuk mengambil data yang berupa text, citra, audio, video, dll. Code Program sebagai berikut : a. Membuat database dengan SQLite3 dan dikoneksikan pada sqlite database yang bersama DB_korea koneksi = sqlite3.connect('DB_korea.db') koneksi.execute(''' CREATE TABLE if not exists VisitKorea (judul TEXT NOT NULL, isi TEXT NOT NULL);''') b. Menentukan link website yang akan di crawl dan disimpan di variable src, kemudian crawling next page apa saja yang akan diambil. Pada web yang akan diambil kali ini, kita mengambil judul, paragraf, dan isi yang ada pada web tersebut. link = artikel[i].find('a')['href'] \u200b page = requests.get(link) \u200b soup = BeautifulSoup(page.content, 'html.parser') \u200b judul = soup.find(class_='title mb-3 text-center').getText() \u200b isi = soup.find(class_='entry-page') \u200b paragraf = isi.findAll('p') \u200b p = ' ' c. kemudian setiap isi, diambil judul dan isi paragraf lantas dicari tag p cek = koneksi.execute(\"SELECT * FROM VisitKorea WHERE judul=?\", (judul,)) cek = cek.fetchall() if (len(cek) == 0): \u200b koneksi.execute('INSERT INTO VisitKorea values (?,?)', (judul, p)); CODE PROGRAM ** artikel = soup.find(class_='row tag-page') artikel = artikel.findAll(class_='col-md-6 col-xl-4') koneksi = sqlite3.connect('DB_korea.db') koneksi.execute(''' CREATE TABLE if not exists VisitKorea (judul TEXT NOT NULL, isi TEXT NOT NULL);''') for i in range(len(artikel)): link = artikel[i].find('a')['href'] page = requests.get(link) soup = BeautifulSoup(page.content, 'html.parser') judul = soup.find(class_='title mb-3 text-center').getText() isi = soup.find(class_='entry-page') paragraf = isi.findAll('p') p = '' for s in paragraf: p+=s.getText() +' ' cek = koneksi.execute(\"SELECT * FROM VisitKorea WHERE judul=?\", (judul,)) cek = cek.fetchall() if (len(cek) == 0): koneksi.execute('INSERT INTO VisitKorea values (?,?)', (judul, p)); Pre-Processing pada tahap pre-processing dilakukan 3 tahapan, yaitu : tahap Stopword Removal (Menghilangkan kata dan tanda baca yang terlalu sering muncul) Stemming (Menggunakan suatu kata menjadi kata dasar) Tokenisasi n-gram (memecah kalimat per kata) Matrik VSM Merupak proses perhitungan kemunculan semua kata yang terdapat pada suata kata Code conn = sqlite3.connect('DB_korea.db') matrix2=[] cursor = conn.execute(\"SELECT* from VisitKOrea\") for row in cursor: \u200b tampung = [] \u200b for i in berhasil: \u200b tampung.append(row[2].lower().count(i)) \u200b #print(tampung) \u200b #print(row[2]) \u200b matrix2.append(tampung) print(matrix2) with open ('data_matrix.csv', newline='', mode='w')as employee_file : \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in matrix : \u200b employee_writer.writerow(i) TF-IDF Digunakan untuk mencari banyak kata yang muncul pada satu dokumen maka IDF digunakan untuk mengetahui mengetahui dokumen yang mana saja yang memiliki kata kata yang sama CODE tf-idf df = list() for d in range (len(matrix[0])): \u200b total = 0 \u200b for i in range(len(matrix)): \u200b if matrix[i][d] !=0: \u200b total += 1 \u200b df.append(total) idf = list() for i in df: \u200b tmp = 1 + log10(len(matrix)/(1+i)) \u200b idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): \u200b tampungBaris = [] \u200b for kolom in range(len(matrix[0])): \u200b tmp = tf[baris][kolom] * idf[kolom] \u200b tampungBaris.append(tmp) \u200b tfidf.append(tampungBaris) with open('tf-idf.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in tfidf: \u200b employee_writer.writerow(i) Code untuk ketiga tahap Stopword Removal (Menghilangkan kata dan tanda baca yang terlalu sering muncul)Stemming (Menggunakan suatu kata menjadi kata dasar) Tokenisasi n-gram (memecah kalimat per kata) Code Program def pearsonCalculate(data, u,v): \u200b \"i, j is an index\" \u200b atas=0; bawah_kiri=0; bawah_kanan = 0 \u200b for k in range(len(data)): \u200b atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) \u200b bawah_kiri += (data[k,u] - meanFitur[u]) 2 \u200b bawah_kanan += (data[k,v] - meanFitur[v]) 2 \u200b bawah_kiri = bawah_kiri 0.5 \u200b bawah_kanan = bawah_kanan 0.5 \u200b return atas/(bawah_kiri * bawah_kanan) def meanF(data): \u200b meanFitur=[] \u200b for i in range(len(data[0])): \u200b meanFitur.append(sum(data[:,i])/len(data)) \u200b return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): \u200b global meanFitur \u200b data = np.array(data) \u200b meanFitur = meanF(data) \u200b u=0 \u200b while u < len(data[0]): \u200b dataBaru=data[:, :u+1] \u200b meanBaru=meanFitur[:u+1] \u200b katadasarBaru=katadasar[:u+1] \u200b v = u \u200b while v < len(data[0]): \u200b if u != v: \u200b value = pearsonCalculate(data, u,v) \u200b if value < threshold: \u200b dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) \u200b meanBaru = np.hstack((meanBaru, meanFitur[v])) \u200b katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) \u200b v+=1 \u200b data = dataBaru \u200b meanFitur=meanBaru \u200b katadasar = katadasarBaru \u200b if u%50 == 0 : print(\"proses : \", u, data.shape) \u200b u+=1 \u200b return katadasar, data Clustering Pengelompokkan data menjadi k-kelompok. K-Means merupakan salah satu metode pengelompokkan data yang berusaha mempartisi data yang ada ke dalam bentuk dua atau lebih. CODE PROGRAM katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf,0.783); kmeans = KMeans(n_clusters=3, random_state=0).fit(tfidf); print(kmeans.labels_) classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print (s_avg) Kesimpulan Menggunakan seleksi fitur dan Clustering dengan Fuzzy K-Mean merupakan metode terbaik dan akurat","title":"Web Content Mining"},{"location":"web_content/#crawling-data-pada-web","text":"Target Website : https://www.visitkorea.or.id/category/informasi-umum/ Yang dibutuhkan utnuk menjalankan program adalah Python 3.6 dengan Library yang di install pada cmd : \u00a7 Request \u00a7 BeautifulSoup4 \u00a7 SQLite 3 (library yang tersedia pada python) \u00a7 Csv (library yang tersedia pada python) \u00a7 Numpy \u00a7 Scipy \u00a7 Scikit-learn \u00a7 Scikit Fuzzy","title":"Crawling Data pada WEB"},{"location":"web_content/#crawling-data-pada-web_1","text":"Mengambil data dari sebuah halaman web dan mengelompokkan data yang sudah di crawling menggunakan metode clustering.Crawling digunakan untuk mengambil data yang berupa text, citra, audio, video, dll. Code Program sebagai berikut : a. Membuat database dengan SQLite3 dan dikoneksikan pada sqlite database yang bersama DB_korea koneksi = sqlite3.connect('DB_korea.db') koneksi.execute(''' CREATE TABLE if not exists VisitKorea (judul TEXT NOT NULL, isi TEXT NOT NULL);''') b. Menentukan link website yang akan di crawl dan disimpan di variable src, kemudian crawling next page apa saja yang akan diambil. Pada web yang akan diambil kali ini, kita mengambil judul, paragraf, dan isi yang ada pada web tersebut. link = artikel[i].find('a')['href'] \u200b page = requests.get(link) \u200b soup = BeautifulSoup(page.content, 'html.parser') \u200b judul = soup.find(class_='title mb-3 text-center').getText() \u200b isi = soup.find(class_='entry-page') \u200b paragraf = isi.findAll('p') \u200b p = ' ' c. kemudian setiap isi, diambil judul dan isi paragraf lantas dicari tag p cek = koneksi.execute(\"SELECT * FROM VisitKorea WHERE judul=?\", (judul,)) cek = cek.fetchall() if (len(cek) == 0): \u200b koneksi.execute('INSERT INTO VisitKorea values (?,?)', (judul, p)); CODE PROGRAM ** artikel = soup.find(class_='row tag-page') artikel = artikel.findAll(class_='col-md-6 col-xl-4') koneksi = sqlite3.connect('DB_korea.db') koneksi.execute(''' CREATE TABLE if not exists VisitKorea (judul TEXT NOT NULL, isi TEXT NOT NULL);''') for i in range(len(artikel)): link = artikel[i].find('a')['href'] page = requests.get(link) soup = BeautifulSoup(page.content, 'html.parser') judul = soup.find(class_='title mb-3 text-center').getText() isi = soup.find(class_='entry-page') paragraf = isi.findAll('p') p = '' for s in paragraf: p+=s.getText() +' ' cek = koneksi.execute(\"SELECT * FROM VisitKorea WHERE judul=?\", (judul,)) cek = cek.fetchall() if (len(cek) == 0): koneksi.execute('INSERT INTO VisitKorea values (?,?)', (judul, p));","title":"Crawling Data pada WEB :"},{"location":"web_content/#pre-processing","text":"pada tahap pre-processing dilakukan 3 tahapan, yaitu : tahap Stopword Removal (Menghilangkan kata dan tanda baca yang terlalu sering muncul) Stemming (Menggunakan suatu kata menjadi kata dasar) Tokenisasi n-gram (memecah kalimat per kata)","title":"Pre-Processing"},{"location":"web_content/#matrik-vsm","text":"Merupak proses perhitungan kemunculan semua kata yang terdapat pada suata kata Code conn = sqlite3.connect('DB_korea.db') matrix2=[] cursor = conn.execute(\"SELECT* from VisitKOrea\") for row in cursor: \u200b tampung = [] \u200b for i in berhasil: \u200b tampung.append(row[2].lower().count(i)) \u200b #print(tampung) \u200b #print(row[2]) \u200b matrix2.append(tampung) print(matrix2) with open ('data_matrix.csv', newline='', mode='w')as employee_file : \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in matrix : \u200b employee_writer.writerow(i)","title":"Matrik VSM"},{"location":"web_content/#tf-idf","text":"Digunakan untuk mencari banyak kata yang muncul pada satu dokumen maka IDF digunakan untuk mengetahui mengetahui dokumen yang mana saja yang memiliki kata kata yang sama CODE","title":"TF-IDF"},{"location":"web_content/#tf-idf_1","text":"df = list() for d in range (len(matrix[0])): \u200b total = 0 \u200b for i in range(len(matrix)): \u200b if matrix[i][d] !=0: \u200b total += 1 \u200b df.append(total) idf = list() for i in df: \u200b tmp = 1 + log10(len(matrix)/(1+i)) \u200b idf.append(tmp) tf = matrix tfidf = [] for baris in range(len(matrix)): \u200b tampungBaris = [] \u200b for kolom in range(len(matrix[0])): \u200b tmp = tf[baris][kolom] * idf[kolom] \u200b tampungBaris.append(tmp) \u200b tfidf.append(tampungBaris) with open('tf-idf.csv', newline='', mode='w') as employee_file: \u200b employee_writer = csv.writer(employee_file, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL) \u200b employee_writer.writerow(katadasar) \u200b for i in tfidf: \u200b employee_writer.writerow(i) Code untuk ketiga tahap Stopword Removal (Menghilangkan kata dan tanda baca yang terlalu sering muncul)Stemming (Menggunakan suatu kata menjadi kata dasar) Tokenisasi n-gram (memecah kalimat per kata) Code Program def pearsonCalculate(data, u,v): \u200b \"i, j is an index\" \u200b atas=0; bawah_kiri=0; bawah_kanan = 0 \u200b for k in range(len(data)): \u200b atas += (data[k,u] - meanFitur[u]) * (data[k,v] - meanFitur[v]) \u200b bawah_kiri += (data[k,u] - meanFitur[u]) 2 \u200b bawah_kanan += (data[k,v] - meanFitur[v]) 2 \u200b bawah_kiri = bawah_kiri 0.5 \u200b bawah_kanan = bawah_kanan 0.5 \u200b return atas/(bawah_kiri * bawah_kanan) def meanF(data): \u200b meanFitur=[] \u200b for i in range(len(data[0])): \u200b meanFitur.append(sum(data[:,i])/len(data)) \u200b return np.array(meanFitur) def seleksiFiturPearson(katadasar, data, threshold): \u200b global meanFitur \u200b data = np.array(data) \u200b meanFitur = meanF(data) \u200b u=0 \u200b while u < len(data[0]): \u200b dataBaru=data[:, :u+1] \u200b meanBaru=meanFitur[:u+1] \u200b katadasarBaru=katadasar[:u+1] \u200b v = u \u200b while v < len(data[0]): \u200b if u != v: \u200b value = pearsonCalculate(data, u,v) \u200b if value < threshold: \u200b dataBaru = np.hstack((dataBaru, data[:, v].reshape(data.shape[0],1))) \u200b meanBaru = np.hstack((meanBaru, meanFitur[v])) \u200b katadasarBaru = np.hstack((katadasarBaru, katadasar[v])) \u200b v+=1 \u200b data = dataBaru \u200b meanFitur=meanBaru \u200b katadasar = katadasarBaru \u200b if u%50 == 0 : print(\"proses : \", u, data.shape) \u200b u+=1 \u200b return katadasar, data","title":"tf-idf"},{"location":"web_content/#clustering","text":"Pengelompokkan data menjadi k-kelompok. K-Means merupakan salah satu metode pengelompokkan data yang berusaha mempartisi data yang ada ke dalam bentuk dua atau lebih. CODE PROGRAM katadasarBaru, fiturBaru = seleksiFiturPearson(katadasar, tfidf,0.783); kmeans = KMeans(n_clusters=3, random_state=0).fit(tfidf); print(kmeans.labels_) classnya = kmeans.labels_ s_avg = silhouette_score(fiturBaru, classnya, random_state=0) print (s_avg)","title":"Clustering"},{"location":"web_content/#kesimpulan","text":"Menggunakan seleksi fitur dan Clustering dengan Fuzzy K-Mean merupakan metode terbaik dan akurat","title":"Kesimpulan"},{"location":"web_structure/","text":"Pagerank merupakan sebuah algoritma yang bertujuan untuk menentukan situs web yang lebih penting (terpopuler). Semakin tinggi pagerank yang dimiliki sebuah halaman web, maka semakin populer halaman web tersebut, karena dengan pagerank yang tinggi yang miliknya itu berarti bahwa situs web tersebut banyak yang bertautan dengan situs web yang lain. Web Structure Mining merupakan salah satu implementasi dari Data Mining, berbeda dengan Web Content Mining yang berfokus pada isi/konten dari suatu website, Web Structure Mining berfokus pada struktur link pada hypertext antar dokumen. Umumnya, Web Structure Mining berfungsi untuk menemukan hubungan antara suatu webpage dengan webpage lain. Hubungan tersebut dapat menentukan apakah kedua webpage tersebut memiliki kemiripan, baik secara struktur maupun konten. Keduanya mungkin saja berada di satu web server yang dibuat oleh satu orang yang sama. Library yang digunakan : import pandas as pd Untuk memudahkan pemrosesan dan eksplorasi data, kita akan mengubah dataset import requests from bs4 import BeautifulSoup Untuk crawling pagerank import networkx as nx import matplotlib.pyplot as plt untuk membuat grafik link yang telah di crawling Code untuk mengambil data def getAllLinks(src): Pencegahan eror apabila link yang diambil mati \u200b try: Get page HTML \u200b page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') # GET all tag <a> tags = soup.findAll(\"a\") links = [] for tag in tags: # Pencegahan eror apabila link tidak memiliki href try: # Get all link link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: #print(\"Error 404 : Page \"+src+\" not found\") return list() Pada code diatas berfungsi mendapatkan semua link pada sebuah website. Pada web structure mining, data yang akan di ambil adalah data yang memiliki hyperlink di dalam tag nya. Di html, link biasa ditampung dalam tag a dengan atribut href. Maka dari itu yang pertama dilakukan adalah mengambil seluruh data yang memiliki tag a di dalamnya, kemudian melakukan filter dengan mengambil tag a yang memiliki atribut href dan mengambil isi dari atribut tersebut untuk ditampung ke dalam list. Code Filter Link def crawl (url, max_deep, show=False, deep=0, done=[]): #returnnya ada di edgelist global edgelist # menambah counter kedalaman deep += 1 # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL(url) #menampilkan proses if not url in done: # crawl semua link links = getAllLinks(url) done.append(url) if show: if deep == 1: print(url) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: # Membentuk format jalan (edge => (dari, ke)) link = simplifiedURL(link) edge = (url,link) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist: edgelist.append(edge) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if (deep != max_deep): crawl(link, max_deep, show, deep, done) Method simplifiedURL berfungsi untuk menyeragamkan format url, menjadi http://urlweb.com/ Lalu, pada method crawl , dilakukan proses crawling secara rekursif. Parameter yang digunakan adalah: url : alamat web awal yang digunakan untuk crawling max_deep : kedalaman maksimal show : menampilkan proses/tidak deep : untuk pengecekan kedalaman. done : untuk pengecekan url yang telah dicrawl. Hal ini untuk mengecilkan waktu crawling. Kedua fungsi diatas akan digabungkan dengan memanggilnya di dalam fungsi crawl. Dimana fungsi ini nantinya akan dipanggil lagi pada main program. root = \"https://www.visitkorea.or.id/\" edgelist = [] nodelist = [root] crawl(root, 3, show=True) edgeListFrame = pd.DataFrame(edgelist, None, (\"From\", \"To\")) Graph g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() Refrensi https://dzone.com/refcardz/data-mining-discovering-and?chapter=3 https://ibara99.github.io/Simple%20Web%20Crawler/Web%20Structure%20Mining/ https://github.com/riivo/pwum https://idbigdata.com/official/belajar-machine-learning-2-loading-dan-eksplorasi-data-dengan-pandas-dan-scikit-learn/ https://id.wikipedia.org/wiki/PageRank","title":"Web Structure Mining"},{"location":"web_structure/#untuk-memudahkan-pemrosesan-dan-eksplorasi-data-kita-akan-mengubah-dataset","text":"import requests from bs4 import BeautifulSoup","title":"Untuk memudahkan pemrosesan dan eksplorasi data, kita akan mengubah dataset"},{"location":"web_structure/#untuk-crawling-pagerank","text":"import networkx as nx import matplotlib.pyplot as plt","title":"Untuk crawling pagerank"},{"location":"web_structure/#untuk-membuat-grafik-link-yang-telah-di-crawling","text":"Code untuk mengambil data def getAllLinks(src): Pencegahan eror apabila link yang diambil mati \u200b try: Get page HTML \u200b page = requests.get(src) # Mengubah html ke object beautiful soup soup = BeautifulSoup(page.content, 'html.parser') # GET all tag <a> tags = soup.findAll(\"a\") links = [] for tag in tags: # Pencegahan eror apabila link tidak memiliki href try: # Get all link link = tag['href'] if not link in links and 'http' in link: links.append(link) except KeyError: pass return links except: #print(\"Error 404 : Page \"+src+\" not found\") return list() Pada code diatas berfungsi mendapatkan semua link pada sebuah website. Pada web structure mining, data yang akan di ambil adalah data yang memiliki hyperlink di dalam tag nya. Di html, link biasa ditampung dalam tag a dengan atribut href. Maka dari itu yang pertama dilakukan adalah mengambil seluruh data yang memiliki tag a di dalamnya, kemudian melakukan filter dengan mengambil tag a yang memiliki atribut href dan mengambil isi dari atribut tersebut untuk ditampung ke dalam list. Code Filter Link def crawl (url, max_deep, show=False, deep=0, done=[]): #returnnya ada di edgelist global edgelist # menambah counter kedalaman deep += 1 # menyamakan format url, agar tidak ada url yg dobel url = simplifiedURL(url) #menampilkan proses if not url in done: # crawl semua link links = getAllLinks(url) done.append(url) if show: if deep == 1: print(url) else: print(\"|\", end=\"\") for i in range(deep-1): print(\"--\", end=\"\") print(\"(%d)%s\" %(len(links),url)) for link in links: # Membentuk format jalan (edge => (dari, ke)) link = simplifiedURL(link) edge = (url,link) # Mengecek jalan, apabila belum dicatat, maka dimasukkan ke list if not edge in edgelist: edgelist.append(edge) # Cek kedalaman, jika belum sampai terakhir, maka crawling. if (deep != max_deep): crawl(link, max_deep, show, deep, done) Method simplifiedURL berfungsi untuk menyeragamkan format url, menjadi http://urlweb.com/ Lalu, pada method crawl , dilakukan proses crawling secara rekursif. Parameter yang digunakan adalah: url : alamat web awal yang digunakan untuk crawling max_deep : kedalaman maksimal show : menampilkan proses/tidak deep : untuk pengecekan kedalaman. done : untuk pengecekan url yang telah dicrawl. Hal ini untuk mengecilkan waktu crawling. Kedua fungsi diatas akan digabungkan dengan memanggilnya di dalam fungsi crawl. Dimana fungsi ini nantinya akan dipanggil lagi pada main program. root = \"https://www.visitkorea.or.id/\" edgelist = [] nodelist = [root] crawl(root, 3, show=True) edgeListFrame = pd.DataFrame(edgelist, None, (\"From\", \"To\")) Graph g = nx.from_pandas_edgelist(edgeListFrame, \"From\", \"To\", None, nx.DiGraph()) pos = nx.spring_layout(g) nx.draw(g, pos) nx.draw_networkx_labels(g, pos, label, font_color=\"w\") plt.axis(\"off\") plt.show() Refrensi https://dzone.com/refcardz/data-mining-discovering-and?chapter=3 https://ibara99.github.io/Simple%20Web%20Crawler/Web%20Structure%20Mining/ https://github.com/riivo/pwum https://idbigdata.com/official/belajar-machine-learning-2-loading-dan-eksplorasi-data-dengan-pandas-dan-scikit-learn/ https://id.wikipedia.org/wiki/PageRank","title":"untuk membuat grafik link yang telah di crawling"}]}